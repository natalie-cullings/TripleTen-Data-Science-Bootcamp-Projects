{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "90174557-ccbc-4d19-b780-5988d67a3706"
    ]
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the text data from the 'imdb_reviews_small.tsv' file. \n",
    "\n",
    "It is a tab-separated values (TSV) file, which means each of the fields are separated by tabs (rather than by commas as you've seen in other Practicum tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/datasets/imdb_reviews_small.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the BERT tokenizer from a pre-trained model which is called `'bert-base-uncased'` in transformers. You can quickly check out an overview of it [here](https://huggingface.co/transformers/pretrained_models.html), and for more details, you can read [here](https://huggingface.co/bert-base-uncased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an example of getting tokens for a given single text. \n",
    "\n",
    "You can use it to process the whole data you've loaded above. As there are already many texts, and you are likely to process them in a loop, the min/max lengths of vectors can be calculated in two ways: either within a loop or after a loop.\n",
    "\n",
    "In the latter case, vectors of numerical identifiers of tokens (`ids`) and attention masks (`attention_mask`) need to be stored in two separate lists. They can be called `ids_list` and `attention_mask_list`, respectively. The first case allows us to avoid building those lists unless you would like to use them for another purpose, e.g. for propagating into a BERT model. It is not required in this task but will be required in the project.\n",
    "\n",
    "Given the above, you may want to combine both ways so that calculate the min/max lengths of vectors for tokens and attention masks and keep the result of the tokenizer for further processing. Please just bear in mind, it does not make much sense to keep vectors longer than 512 elements as this is the max length of vectors that BERT can accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts to tokens\n",
    "text = 'It is very handy to use transformers'\n",
    "\n",
    "# adding this trick to supress warnings of lengthy outputs\n",
    "# we do not normally need to, but in this case we'd like to explore \n",
    "# what is the max length of ids for our set of reviews \n",
    "# therefore we do not truncate the output (ids) to the max_length\n",
    "# with the parameters max_length=max_length and truncation=True\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "        \n",
    "ids = tokenizer.encode(text.lower(), add_special_tokens=True)\n",
    "\n",
    "# padding (appending zeroes to the vector to make its length equal to n)\n",
    "n = 512\n",
    "padded = np.array(ids[:n] + [0]*(n - len(ids)))\n",
    "\n",
    "# creating the attention mask to distinguish tokens we are interested in\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2009, 2003, 2200, 18801, 2000, 2224, 19081, 102]\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  2009  2003  2200 18801  2000  2224 19081   102     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compose your code to tokenize the loaded text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_bert(texts):\n",
    "    \n",
    "    ids_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    min_tokenized_text_length = 1e7\n",
    "    max_tokenized_text_length = 0\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize the text\n",
    "        ids = tokenizer.encode(text.lower(), add_special_tokens=True, max_length=512, truncation=True)\n",
    "        \n",
    "        # Identify min and max lengths\n",
    "        min_tokenized_text_length = min(min_tokenized_text_length, len(ids))\n",
    "        max_tokenized_text_length = max(max_tokenized_text_length, len(ids))\n",
    "        \n",
    "        # Pad the tokenized text to 512 tokens\n",
    "        padded = np.array(ids + [0] * (512 - len(ids)))\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        \n",
    "        ids_list.append(padded)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "    \n",
    "    print(f'The minimum length of vectors: {min_tokenized_text_length}')\n",
    "    print(f'The maximum length of vectors: {max_tokenized_text_length}')\n",
    "        \n",
    "    return ids_list, attention_mask_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the tokenizer for the whole data. It can take some time as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum length of vectors: 18\n",
      "The maximum length of vectors: 512\n"
     ]
    }
   ],
   "source": [
    "ids_list, attention_mask_list = tokenize_with_bert(texts=data['review'])"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 956,
    "start_time": "2024-09-14T18:25:42.494Z"
   },
   {
    "duration": 2,
    "start_time": "2024-09-14T18:25:46.673Z"
   },
   {
    "duration": 5,
    "start_time": "2024-09-14T18:27:10.872Z"
   },
   {
    "duration": 89,
    "start_time": "2024-09-14T18:27:16.198Z"
   },
   {
    "duration": 1763,
    "start_time": "2024-09-14T18:27:25.669Z"
   },
   {
    "duration": 403,
    "start_time": "2024-09-14T18:29:41.083Z"
   },
   {
    "duration": 4,
    "start_time": "2024-09-14T18:29:44.870Z"
   },
   {
    "duration": 3,
    "start_time": "2024-09-14T18:29:48.936Z"
   },
   {
    "duration": 4,
    "start_time": "2024-09-14T18:29:53.531Z"
   },
   {
    "duration": 3,
    "start_time": "2024-09-14T18:29:57.117Z"
   },
   {
    "duration": 18,
    "start_time": "2024-09-14T18:42:32.629Z"
   },
   {
    "duration": 17,
    "start_time": "2024-09-14T18:42:41.372Z"
   },
   {
    "duration": 4,
    "start_time": "2024-09-14T18:42:45.220Z"
   },
   {
    "duration": 12782,
    "start_time": "2024-09-14T18:42:55.130Z"
   },
   {
    "duration": 16,
    "start_time": "2024-09-14T18:43:57.736Z"
   },
   {
    "duration": 4,
    "start_time": "2024-09-14T18:47:55.268Z"
   },
   {
    "duration": 12635,
    "start_time": "2024-09-14T18:48:01.293Z"
   },
   {
    "duration": 2351,
    "start_time": "2024-09-14T18:48:20.087Z"
   },
   {
    "duration": 22,
    "start_time": "2024-09-14T18:48:34.295Z"
   },
   {
    "duration": 17,
    "start_time": "2024-09-14T18:48:49.478Z"
   },
   {
    "duration": 4,
    "start_time": "2024-09-14T18:48:55.456Z"
   },
   {
    "duration": 4,
    "start_time": "2024-09-14T18:58:36.540Z"
   },
   {
    "duration": 8,
    "start_time": "2024-09-14T18:58:43.199Z"
   },
   {
    "duration": 4,
    "start_time": "2024-09-14T18:59:09.396Z"
   },
   {
    "duration": 12648,
    "start_time": "2024-09-14T18:59:12.712Z"
   },
   {
    "duration": 5,
    "start_time": "2024-09-14T19:00:07.195Z"
   },
   {
    "duration": 4,
    "start_time": "2024-09-14T19:00:24.680Z"
   },
   {
    "duration": 5,
    "start_time": "2024-09-14T19:00:41.985Z"
   },
   {
    "duration": 4,
    "start_time": "2024-09-14T19:01:25.048Z"
   },
   {
    "duration": 931,
    "start_time": "2024-09-14T19:01:36.485Z"
   },
   {
    "duration": 56,
    "start_time": "2024-09-14T19:01:37.419Z"
   },
   {
    "duration": 1723,
    "start_time": "2024-09-14T19:01:37.476Z"
   },
   {
    "duration": 3,
    "start_time": "2024-09-14T19:01:39.202Z"
   },
   {
    "duration": 4,
    "start_time": "2024-09-14T19:01:39.207Z"
   },
   {
    "duration": 5,
    "start_time": "2024-09-14T19:01:39.212Z"
   },
   {
    "duration": 3,
    "start_time": "2024-09-14T19:01:39.220Z"
   },
   {
    "duration": 5,
    "start_time": "2024-09-14T19:01:39.225Z"
   },
   {
    "duration": 12750,
    "start_time": "2024-09-14T19:01:39.232Z"
   },
   {
    "duration": 3,
    "start_time": "2024-09-14T19:01:51.985Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
